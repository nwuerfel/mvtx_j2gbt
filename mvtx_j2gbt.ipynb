{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ce1894",
   "metadata": {},
   "source": [
    "<h2>A JSON to Bytestream converter for Testing the MVTX AI Heavy Flavor Trigger Firmware Block </h2>\n",
    "<p>nwuerfel@umich.edu - Noah Wuerfel /* ~AP AP AP~ goblu */</p>\n",
    "\n",
    "<h3>Purpose and Motivation</h3>\n",
    "<p> A LANL-led project to test the firmware implementation of an AI-based heavy flavor trigger for the inner tracking system of the sPHENIX experiment. Currently, the collaboration simulation outputs a .json file containing hit information in the format of pixel, chip, etc which is used by our collaborators to train the AI trigger using software tools. A concurrent effort is underway in the collaboration to utilize the hls4ml package [^1] to generate a firmware block implementation of the trained, software AI engine. To test the firmware implementation of the trigger, the LANL group plans to use a pair of FPGA development boards (Xilinx KCU105 and VC709) to emulate the optical links and data transfer of the detector electronics. The Xilinx XDMA direct memory access IP has been used to establish virtual file references for the DDR3 memory on the boards. This software takes the output of our simulation in .json format and converts it to a bytestream - to be loaded on the KCU105 and send via optical link to the VC709 which will house the AI trigger block implementation where final results will be read out over DMA.</p> \n",
    "\n",
    "<h3>An overview of the Hardware Setup</h3>\n",
    "<p>Ultimately, this software is approximating the performance of the front end electronics to convert pixel and chip hits to the bytestream those detectors would actually produce during runtime. The Monolithic Active Pixel Sensor Based Vertex Detector (MVTX) is comprised of a number of \"staves\" which, in the inner layers, hold 9 detector chips called ALPIDES which are (512x1024) arrays of sensitive silicon pixel detectors. Fast tracking data from these detectors is used to generate physics triggers for detector readout. The ALPIDE chips are divided horozontally into 32 readout regions, each region contains 16 priority encoders which readout 1024 pixels each in double columns. Pixel data is arranged according to the ALPIDE data format described later in the notebook. 8 Staves each send 3 chips worth of data each on 3 parallel lines (for a total of 72 ALPIDEs) to each Front End Exchange unit (FELIX) where we plan on sending a copy to the AI engine. The data protocol from Stave to FELIX / AI block is the \"GBT format\", sets of 80 bit data words defined by the sPHENIX MVTX data format. There will be more in depth discriptions of each data format near the respective classes in code. </p>\n",
    "\n",
    "<h3>A List of Classes</h3>\n",
    "\n",
    "- <b>Data Classes</b>\n",
    "    - Hit\n",
    "    - RawDataHeaderWord\n",
    "    - StartPacketWord\n",
    "    - EndPacketWord\n",
    "    - ITSHeaderWord\n",
    "    - TriggerDataHeader\n",
    "    - TriggerDataTrailer\n",
    "- <b>Classes Representing Electronics or Helpers</b>\n",
    "    - StaveInfo\n",
    "    - GBTLink\n",
    "    - AlpideInfo\n",
    "    - RegionInfo\n",
    "\n",
    "\n",
    "<h3>Case and Code Standards</h3>\n",
    "<p>Typically I try to maintain consistent code standards and prefer camelCase for my variable and function names with ALLCAPS for preprocessor directives (or in python, constant globals or class features) but in this case I'm trying to be consistent with my own coding preferences and names which match documentation. As much as possible, I have tried to maintain the original casing of the various documented bytefields. For example, many classes have an \"isEmpty\" method but the valid alpide datawords have fields such as \"encoder_id\". Oh well...</p> \n",
    "\n",
    "[^1]: J. Duarte et al., “Fast inference of deep neural networks in FPGAs for particle physics”, JINST 13 P07027 (2018), arXiv:1804.06913. ; S. Summers et al., “Fast inference of boosted decision trees in FPGAs for particle physics”, arXiv:2002.02534 ; G. Di Guglielmo et al., “Compressing deep neural networks on FPGAs to binary and ternary precision with hls4ml”, arXiv:2003.06308 - https://fastmachinelearning.org/hls4ml/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a924d",
   "metadata": {},
   "source": [
    "<h1>Helper Functions</h1>\n",
    "\n",
    "<p>Here we have various helper functions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f11e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns index of a colpixel inside region\n",
    "def pEncoderID(pixel_z):\n",
    "    return (int(pixel_z/2) % 16)\n",
    "\n",
    "# returns encoder region given a colpixel\n",
    "def pEncoderRegion(pixel_z):\n",
    "    return (int(pixel_z/32))\n",
    "\n",
    "# checks even odd for number\n",
    "def isEven(num):\n",
    "    if(num%2 == 0):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "#return unique elements of array    \n",
    "def unique(x):\n",
    "    x = np.array(x)\n",
    "    return np.unique(x)\n",
    "\n",
    "# returns encoder addresse given pixel row and column\n",
    "def pixelAddr(row, column):\n",
    "    doubleColIdx = (column % 2)\n",
    "    #C0\n",
    "    if(doubleColIdx == 0):\n",
    "        if(isEven(row)):\n",
    "            return 2*row\n",
    "        else:\n",
    "            return 2*(row+1) -1\n",
    "    #C1\n",
    "    else:\n",
    "        if(isEven(row)):\n",
    "            return 2*row + 1\n",
    "        else:\n",
    "            return 2*row\n",
    "        return   \n",
    "\n",
    "# returns a bytearray from a binary string (requires byte alignment of arguments)\n",
    "def toBytes(Data):\n",
    "    assert((len(Data) % 8) == 0)\n",
    "    ByteData = bytearray()\n",
    "    for idx, bit in enumerate(Data):\n",
    "        if ((idx % 8) == 0):\n",
    "            byte = Data[idx: idx + 8]\n",
    "            byte = int(byte,2)\n",
    "            byte = bytearray([byte])\n",
    "            ByteData = ByteData + byte  \n",
    "    return ByteData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8621548",
   "metadata": {},
   "source": [
    "<h1>Helper Functions Demos</h1>\n",
    "\n",
    "<p>Here we have various helper functions demonstrated.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrates priority encoder mappings\n",
    "columns = range(0,1024)\n",
    "encoderIds = [pEncoderID(x) for x in columns]\n",
    "for i in range(0,16):\n",
    "    for j in range(0,2):\n",
    "        print('{:0>4}'.format(pixelAddr(i,j)) , end=\" \")\n",
    "    print()\n",
    " \n",
    "print()  \n",
    "    \n",
    "# Demonstrates toBytes:\n",
    "bitstring = '11100000'\n",
    "byteData = toBytes(bitstring)\n",
    "print(bitstring)\n",
    "print(byteData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6c2a4",
   "metadata": {},
   "source": [
    "<h1>Hit format from JSON</h1>\n",
    "<p>The Hit class is a container for the basic input from the .json simulation output. Information comes in the format of: Layer, Stave, Chip, Pixel_x and Pixel_z which are mapped from ALPIDE row and column in simulation as below:</p>\n",
    "\n",
    "```\n",
    "{\n",
    "    unsigned int pixel_x = MvtxDefs::getRow(hitkey);\n",
    "\tunsigned int pixel_z = MvtxDefs::getCol(hitkey);\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f83f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hit:\n",
    "    def __init__(self, TrkID, Layer, Stave, Chip, Pixel_x, Pixel_z):\n",
    "        self.MVTXTrkID = TrkID\n",
    "        self.Layer = Layer\n",
    "        self.Stave = Stave\n",
    "        self.Chip = Chip\n",
    "        self.Pixel_x = Pixel_x\n",
    "        self.Pixel_z = Pixel_z\n",
    "        \n",
    "    def Print(self):\n",
    "        print(\"Hit Info-- Pixel_x: \", self.Pixel_x, \" Pixel_z: \", self.Pixel_z, \n",
    "                  \" pixelAddr: \", pixelAddr(self.Pixel_x,self.Pixel_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f9c0d",
   "metadata": {},
   "source": [
    "<h1>DATA GBT Words</h1>\n",
    "<p>Self-explanatory</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f61710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the RDH \n",
    "class RawDataHeaderWord:\n",
    "    \n",
    "    # defined fields\n",
    "    ReservedHead = f'{0:08b}'\n",
    "    ReservedHeadLarge = f'{0:032b}'\n",
    "    ReservedMiddle = f'{0:020b}'\n",
    "    \n",
    "    #TODO\n",
    "    # partially defined for mvtx?\n",
    "    DetectorField = f'{0:032b}'\n",
    "    \n",
    "    SourceID = f'{32:08b}'\n",
    "    \n",
    "    #TODO\n",
    "    FEEID = f'{0:016b}'\n",
    "    \n",
    "    HeaderSize = f'{64:08b}'\n",
    "    HeaderVersion = f'{8:08b}'\n",
    "    \n",
    "    #TODO\n",
    "    #Trigger message BCO from GTM ?\n",
    "    GTMBCO = f'{0:048b}'\n",
    "\n",
    "    #TODO\n",
    "    #Trigger message bunch crossing LHC clock from RU ?\n",
    "    LHCBC = f'{0:012b}'\n",
    "    \n",
    "    #TODO\n",
    "    # when 0x1 the packet is moved forward with priority?\n",
    "    PriorityBit = f'{0:08b}'\n",
    "    \n",
    "    #TODO\n",
    "    #Counter to keep track of CRU Data packet in same heartbeat\n",
    "    PagesCounter = f'{0:016b}'\n",
    "    \n",
    "    #TODO\n",
    "    #Trigger type bit set by Felix at HB trigger...\n",
    "    TrgType= f'{0:032b}'\n",
    "    \n",
    "    def __init__(self, StopBit = 0, PagesCounter = 0):\n",
    "        self.StopBit = f'{StopBit:08b}'\n",
    "        self.PagesCounter = f'{PagesCounter:016b}'\n",
    "        self.GBT0 = (self.ReservedHead + self.DetectorField + self.SourceID + self.FEEID \n",
    "                     + self.HeaderSize + self.HeaderVersion)\n",
    "        self.GBT1 = self.ReservedHead + self.GTMBCO + self.ReservedMiddle + self.LHCBC\n",
    "        self.GBT2 = self.ReservedHeadLarge + self.PriorityBit + self.StopBit + self.PagesCounter + self.TrgType\n",
    "        self.Data = self.GBT0 + self.GBT1 + self.GBT2 \n",
    "        self.ByteData = toBytes(self.Data)\n",
    "                \n",
    "    def Print(self):\n",
    "        print (self.Data)\n",
    "        \n",
    "    def PrintBytes(self):\n",
    "        print( ' '.join( '{:02x}'.format(x) for x in self.ByteData ) )\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example RDH\n",
    "a = RawDataHeaderWord()\n",
    "b = RawDataHeaderWord(1,3)\n",
    "a.PrintBytes()\n",
    "b.PrintBytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d514743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartPacketWord:\n",
    "    #DV is internal to CRU I think\n",
    "    #DV = f'{0:01b}'\n",
    "    ControlCode = f'{1:04b}'\n",
    "    Length = f'{0:016b}'\n",
    "    TTSBusy = f'{0:016b}'\n",
    "    Reserved = f'{0:044b}'\n",
    "    Data = ControlCode + Length + TTSBusy + Reserved\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ByteData = toBytes(self.Data)\n",
    "    \n",
    "    def Print(self):\n",
    "        print(self.Data)   \n",
    "    \n",
    "    def PrintBytes(self):\n",
    "        print( ' '.join( '{:02x}'.format(x) for x in self.ByteData ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndPacketWord:\n",
    "    #DV = f'{0:01b}'\n",
    "    ControlCode = f'{2:04b}'\n",
    "    Length = f'{0:016b}'\n",
    "    Checksum = f'{0:032b}'\n",
    "    EndFlag = f'{0:01b}'\n",
    "    Reserved = f'{0:027b}'\n",
    "    Data = ControlCode + Length + Checksum + EndFlag + Reserved\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ByteData = toBytes(self.Data)  \n",
    "        \n",
    "    def Print(self):\n",
    "        print(self.Data)\n",
    "        \n",
    "    def PrintBytes(self):\n",
    "        print( ' '.join( '{:02x}'.format(x) for x in self.ByteData ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410e7c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ITSHeaderWord:\n",
    "    ID = '11100000'\n",
    "    Reserved = f'{0:044b}'\n",
    "    \n",
    "    #TODO assumed all lanes active now\n",
    "    active_lanes = '1' * 28\n",
    "    #active_lanes = f'{0:028b}'\n",
    "    \n",
    "    Data = ID + Reserved + active_lanes\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ByteData = toBytes(self.Data)\n",
    "\n",
    "    def PrintBytes(self):\n",
    "        print( ' '.join( '{:02x}'.format(x) for x in self.ByteData ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriggerDataHeader:\n",
    "    ID = '11101000'\n",
    "    \n",
    "    # TODO not in sim\n",
    "    GTM_BCO = f'{0:040b}'\n",
    "    \n",
    "    Reserved_1 = f'{0:04b}'\n",
    "    \n",
    "    # TODO not in sim\n",
    "    LHC_BC = f'{0:012b}'\n",
    "    \n",
    "    Reserved_2 = '0'\n",
    "    \n",
    "    # Continuation needs to be initialized\n",
    "    \n",
    "    # NoData needs to be initialized\n",
    "    \n",
    "    # Triggered mode - internal trigger not used\n",
    "    InternalTrigger = '0'\n",
    "    \n",
    "    # TODO not in sim\n",
    "    TriggerType = f'{0:012b}'\n",
    "    \n",
    "    def __init__(self, Continuation = '0', NoData = '0'):\n",
    "        self.Continuation = Continuation\n",
    "        self.NoData = NoData\n",
    "        self.ByteData = toBytes(self.ID + self.GTM_BCO + self.Reserved_1 + self.LHC_BC + \n",
    "                                self.Reserved_2 + self.Continuation + self.NoData \n",
    "                                    + self.InternalTrigger + self.TriggerType)\n",
    "\n",
    "    def PrintBytes(self):\n",
    "        print( ' '.join( '{:02x}'.format(x) for x in self.ByteData ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daef2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriggerDataTrailer:\n",
    "    ID = '11110000'\n",
    "    Reserved_1 = f'{0:04b}'\n",
    "    \n",
    "    # TODO assume all lanes work fine\n",
    "    lane_starts_violation = '0'\n",
    "    \n",
    "    Reserved_2 = '0'\n",
    "    \n",
    "    # TODO\n",
    "    transmission_timeout = '0'\n",
    "    \n",
    "    # TODO \n",
    "    packet_done = '1'\n",
    "    \n",
    "    Reserved_3 = f'{0:08b}'\n",
    "    \n",
    "    # TODO assume all lanes work fine\n",
    "    lane_status = f'{0:056b}'\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ByteData = toBytes(self.ID + self.Reserved_1 + self.lane_starts_violation + self.Reserved_2 \n",
    "                                    + self.transmission_timeout + self.packet_done \n",
    "                                        + self.Reserved_3 + self.lane_status)\n",
    "        \n",
    "    def PrintBytes(self):\n",
    "        print( ' '.join( '{:02x}'.format(x) for x in self.ByteData ) )    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e86fd4",
   "metadata": {},
   "source": [
    "<h1>Stave and GBT Formatting</h1>\n",
    "<p>Following are the StaveInfo and GBTLink classes which package the data from the ALPIDES on the frontends into the GBT format expected over the optical links at FELIX / AI Engine. The StaveInfo class simply packages the data on the ALPIDES for the GBTLink class which does the real work of formatting GBT words. Data packets on the GBT lines are formatted into pages of the following format: Start of Packet, Raw Data Header Word, ITS Header Word, Trigger Data Header, ITS Data, Trigger Data Trailer, End of Packet. The Pages have a 512 word limit excluding the start and end words, and for now I also copy the other data words in new pages if needed as well. In our case, the ITS data is the ALPIDE formatted data, described near the alpide class. Data from the Alpies is split into 9 byte chunks and sent LSB first along with an appended MSB called the RU_GBT_ID_WORD containing geographic information about the chip dat is generated from.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaveInfo:\n",
    "    def __init__(self, Layer, Stave, AlpideList):\n",
    "        self.Layer = Layer\n",
    "        self.Stave = Stave\n",
    "        self.AlpideList = AlpideList.copy()\n",
    "        self.GBTLinks = []\n",
    "        \n",
    "        \n",
    "    def formatData(self):\n",
    "        # the alpide chips are split on 3 separate GBT lines coming from the Stave going to the RU\n",
    "        # the chips are currently arranged on the lines in geographic order; 0-2, 3-5, 6-8\n",
    "        \n",
    "       #print(\"stave to link alpide lists...\")\n",
    "       # for alpide in self.AlpideList:\n",
    "       #     print(alpide.Chip)\n",
    "        self.GBTLinks.append(GBTLink(self.AlpideList[0:3]))\n",
    "        self.GBTLinks.append(GBTLink(self.AlpideList[3:6]))\n",
    "        self.GBTLinks.append(GBTLink(self.AlpideList[6:9]))\n",
    "        for link in self.GBTLinks:\n",
    "            link.formatData()\n",
    "        \n",
    "    def Print(self):\n",
    "        print(\"|------- Stave Info --------\")\n",
    "        print( \"Layer: \", self.Layer, \" Stave: \", self.Stave, \" Chip List... \")\n",
    "        print()\n",
    "        for Alpide in self.AlpideList:\n",
    "            Alpide.Print()\n",
    "    \n",
    "    def PrintChips(self):\n",
    "        print(\"|------- Stave ByteInfo --------\")\n",
    "        print( \"Layer: \", self.Layer, \" Stave: \", self.Stave, \" Chip List... \")\n",
    "        print()\n",
    "        for Alpide in self.AlpideList:\n",
    "            Alpide.PrintBytes()\n",
    "            \n",
    "    def PrintGBTLinks(self):\n",
    "        print(\"|------ Stave GBTInfo --------\")\n",
    "        for idx, link in enumerate(self.GBTLinks):\n",
    "            print(\"GBT LINK: \" + str(idx) + \" Bytes:\")\n",
    "            link.PrintBytes()\n",
    "            \n",
    "class ReadoutUnit:\n",
    "    def __init__(self, Stave):\n",
    "        self.Stave = Stave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc8c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBTLink:\n",
    "    \n",
    "    MAX_PAGE_SIZE = 512\n",
    "    CHIP_PER_LINK = 3\n",
    "    BIT_PER_GBT = 80\n",
    "    BYTE_PER_GBT = 10\n",
    "    GBT_IN_RDH = 3\n",
    "    \n",
    "    def __init__(self, AlpideList):\n",
    "        self.AlpideList = AlpideList\n",
    "        self.ByteData = bytearray([])\n",
    "        \n",
    "    def formatData(self):\n",
    "        alpideBytes = []\n",
    "        linkMVTXData = []\n",
    "        totalDataWords = 0\n",
    "        \n",
    "        # first let's build ITS data words to get a count for paging later\n",
    "        for alpide in self.AlpideList:\n",
    "            \n",
    "            localMVTXData = []\n",
    "            # empty chips send the EMPTY CHIP TRAILER, not NOTHING...\n",
    "            byteData = alpide.getFormattedData()\n",
    "            \n",
    "            #inner barrel ITS for MVTX is '001' + 5'b chipid \n",
    "            RU_GBT_WORD_ID = '001' + f'{alpide.Chip:05b}'\n",
    "            RU_GBT_WORD_ID = toBytes(RU_GBT_WORD_ID)\n",
    "            \n",
    "            #break chip data into its data format RU_GBT_WORD + 9 AlpideBytes\n",
    "            #note that the alpide data is already 9 byte aligned\n",
    "            byteNo = len(byteData)\n",
    "            assert((byteNo % 9) == 0)\n",
    "            for i in range(0,byteNo):\n",
    "                if (i % 9 == 0):\n",
    "                    datawords = byteData[i:i+9]\n",
    "                    #byte data gets send LSB first...\n",
    "                    datawords = datawords[::-1]\n",
    "                    mvtxWord = RU_GBT_WORD_ID + datawords\n",
    "                    localMVTXData.append(mvtxWord)\n",
    "            linkMVTXData.append(localMVTXData)\n",
    "            totalDataWords = totalDataWords + len(localMVTXData)\n",
    "        \n",
    "        # need to count pages to build packets.. Pagesize is 512\n",
    "        # for inner barrel staves, the 3 chip links get RR into the CRU packets\n",
    "        # page includes 3 RDH GBT words, plust status and data words\n",
    "        # our dat looks like: SOP, IHW, TDH, ITSDATA, TDT, EOP split as many packets as needed for paging\n",
    "        \n",
    "        #TODO Probably don't understand the paging properly wrt the TDT TDH IWH\n",
    "        # not sure if I get one set of IHW TDH TDT per trigger or if we get a set per page...\n",
    "        #determine num of pages... we have data + 3 RDH words per page + IWH,TDH,TDT per page\n",
    "        numPages = math.ceil(totalDataWords/(self.MAX_PAGE_SIZE - self.GBT_IN_RDH - 3))\n",
    "        \n",
    "        #common data words that don't change...\n",
    "        SOP = StartPacketWord()\n",
    "        SOPBytes = SOP.ByteData\n",
    "        EOP = EndPacketWord()\n",
    "        EOPBytes = EOP.ByteData\n",
    "        IHW = ITSHeaderWord()\n",
    "        IHWBytes = IHW.ByteData\n",
    "        \n",
    "        # keep track of idx since we round robin our data out...\n",
    "        # not guarunteed to have hits on all three of our chips\n",
    "        chipIdx = [0]*self.CHIP_PER_LINK\n",
    "        chipTotWords = []\n",
    "        for i in range(0,self.CHIP_PER_LINK):\n",
    "            chipTotWords.append(len(linkMVTXData[i]))\n",
    "        \n",
    "        # build packets...\n",
    "        for i in range(0,numPages):\n",
    "            dataPacket = bytearray([])\n",
    "            \n",
    "            # set stopbit if needed in header\n",
    "            if(i == (numPages-1)):\n",
    "                RDH = RawDataHeaderWord(1,i)\n",
    "            else:\n",
    "                RDH = RawDataHeaderWord(0,i)\n",
    "            RDHBytes = RDH.ByteData\n",
    "            \n",
    "            #set continuation if we're in a new page\n",
    "            if( i == 0 ):\n",
    "                TDH = TriggerDataHeader()\n",
    "            else:\n",
    "                TDH = TriggerDataHeader(1,0)\n",
    "            TDHBytes = TDH.ByteData\n",
    "            \n",
    "            TDT = TriggerDataTrailer()\n",
    "            TDTBytes = TDT.ByteData\n",
    "            \n",
    "            dataPacket = dataPacket + SOPBytes + RDHBytes + IHWBytes + TDHBytes\n",
    "            \n",
    "            # RR data from the Alpides until we fill page\n",
    "            # previously misunderstood that simming time ordering in RR would involve random sampling to\n",
    "            # emulate time ordering differences in RR, instead I just fill cyclically\n",
    "            for j in range(0, (self.MAX_PAGE_SIZE - self.GBT_IN_RDH - 3)):\n",
    "                \n",
    "                \n",
    "                lastChip = 0\n",
    "                chipSelected = False\n",
    "                dataToRead = False\n",
    "                \n",
    "                # check if any chips have data left to read...\n",
    "                for k in range(0,len(linkMVTXData)):\n",
    "                    if(chipIdx[k] < chipTotWords[k]):\n",
    "                        dataToRead = True\n",
    "                        break\n",
    "                        \n",
    "                if (not dataToRead):\n",
    "                    break\n",
    "                \n",
    "                # cyclically select chip with data to RR if it has data left to read                \n",
    "                while(not chipSelected):\n",
    "                    chipToRead = (lastChip) % self.CHIP_PER_LINK\n",
    "                    lastChip = lastChip + 1\n",
    "                    # check last index of chip data to make sure we're in range\n",
    "                    if (chipIdx[chipToRead] < chipTotWords[chipToRead]):\n",
    "                        chipSelected = True\n",
    "                        \n",
    "                # take a word from cyclically selected chip and add to packet\n",
    "                chipDataArray = linkMVTXData[chipToRead]\n",
    "                ITSDataWordBytes = chipDataArray[chipIdx[chipToRead]]\n",
    "                dataPacket = dataPacket + ITSDataWordBytes\n",
    "                \n",
    "                # increment the idx for that chip\n",
    "                chipIdx[chipToRead] = chipIdx[chipToRead] + 1\n",
    "                \n",
    "            # add the TDT and EOP\n",
    "            dataPacket = dataPacket + TDTBytes + EOPBytes\n",
    "            \n",
    "            # add page to data\n",
    "            self.ByteData = self.ByteData + dataPacket\n",
    "            \n",
    "            \n",
    "    def PrintBytes(self):\n",
    "        print( ' '.join( '{:02x}'.format(x) for x in self.ByteData ) )\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60554c36",
   "metadata": {},
   "source": [
    "<h5>Alpide valid data words:</h5>\n",
    "\n",
    "| Data Word   | Length (Bits) | Value (binary)                                  |\n",
    "| :----------- | :------------- | :-----------------------------                   |\n",
    "| CHIP HEADER        | 16             | 1010<chip_id[3:0]><BUNCH_COUNTER_FOR_FRAME[10:3]>|\n",
    "| CHIP TRAILER        | 8             | 1011<readout_flags[3:0]>|\n",
    "| CHIP EMPTY FRAME        | 16           | 1110<chip_id[3:0]><BUNCH_COUNTER_FOR_FRAME[10:3]>|\n",
    "| REGION HEADER        | 8             | 110<region_id[4:0]>|\n",
    "| IDLE        | 8             | 1111_1111                                       |\n",
    "| Data Short  | 16            | 01<encoder_id[3:0]><addr[9:0]>                  |\n",
    "| Data Long   | 24            | 00<encoder_id[3:0]><addr[9:0]>_0_<hit_map[6:0]> |\n",
    "\n",
    "<h5> DATA SHORT </h5>\n",
    "<p> encoder_id is the index of the priority encoder inside a region and addr is the pixel hit index from the pixel encoder. From part of the description in DATA LONG, it seems that the pixel with lowest address gets read out if there are multiple hits -- TODO CONFIRM</p>\n",
    "\n",
    "<h5> DATA LONG </h5>\n",
    "<p> only used if clustering is enabled - encoder_id and addr are the same as DATA SHORT, containing the geographical information of the first pixel (lowest address) the hit_map[6:0] contains the cluster shape information as a bitmap - a bit in the hit_map is set for any active pixel among the 7 after (based on PE addr) the one in the addr[9:0] field.</p>\n",
    "\n",
    "<h5> Row and Column mappings: </h5>\n",
    "| C0 | C1 |\n",
    "| --- | ---|\n",
    "| 0 | 1|\n",
    "| 3 | 2|\n",
    "|---|---|\n",
    "| 4 | 5 |\n",
    "| 7 | 6 |\n",
    " |  |\n",
    "\n",
    "<p>In C0 of double column, the even rows have address: 2*row, and the odd rows have address (row+1)*2 -1. In C1 of double column, the even rows have address 2*row+1 and the odd rows have 2*row </p>\n",
    "\n",
    "<h5> Clustering readout and DATA LONG </h5>\n",
    "\n",
    "<h5> Data readout </h5>\n",
    "<p> Region data frames are sent sequentially in ascending order - region header only comes from regions with pixelhit information </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlpideInfo:\n",
    "    def __init__(self,Layer,Stave,Chip,HitList):\n",
    "        self.Layer = Layer\n",
    "        self.Stave = Stave\n",
    "        self.Chip = Chip\n",
    "        self.HitList = HitList.copy()\n",
    "        self.regionData = []\n",
    "        self.byteData = bytearray([])\n",
    "        self.formatted = False\n",
    "        \n",
    "        # empty chip\n",
    "        if (self.Layer == -1):\n",
    "            # empty chip frame\n",
    "            # for now there's no Bunch cross counter...\n",
    "            self.byteData = '1110' + f'{self.Chip:04b}' + f'{0:08b}'\n",
    "            self.byteData = toBytes(self.byteData)\n",
    "            # pad with IDLE word to 9 byte alignment for later...\n",
    "            self.byteData = self.byteData + bytearray([255]*7)\n",
    "            self.formatted = True\n",
    "        \n",
    "    def getFormattedData(self):\n",
    "        if (not self.formatted):\n",
    "            self.formatData()\n",
    "        return self.byteData\n",
    "        \n",
    "    def formatData(self):\n",
    "        if (self.formatted):\n",
    "            return\n",
    "        \n",
    "        localRegionData=[]\n",
    "        PEHitList = []\n",
    "        if(self.HitList):\n",
    "            \n",
    "            #format by region (32)\n",
    "            for i in range(0,32):\n",
    "                regionHits = [x for x in self.HitList if pEncoderRegion(x.Pixel_z) == i]\n",
    "                regionHits = sorted(regionHits, key = lambda hit : hit.Pixel_z)\n",
    "                data = []\n",
    "                #format by priority Encoder (16 per region)\n",
    "                for PEIdx in range(0,16):\n",
    "                    PEHits = [x for x in regionHits if pEncoderID(x.Pixel_z) == PEIdx]\n",
    "                    PEHits = sorted(PEHits, key = lambda hit: pixelAddr(hit.Pixel_x, hit.Pixel_z))\n",
    "                    PEHitList.append(PEHits)\n",
    "                    \n",
    "                    # build data words... \n",
    "                    # misunderstood documentation: IF clustering is NOT enabled, we do data short for each hit\n",
    "                    # not just the lowest pixel\n",
    "                    # in the case that clustering IS enabled, but there's no cluster, we send only DATA_SHORT \n",
    "                    # rather than emtpy bitmap\n",
    "                    if PEHits:\n",
    "                        # if clustering we only need lowest pix addr hit\n",
    "                        if(EnableClustering):\n",
    "                            lowPixAddrHit = PEHits[0]\n",
    "                            #build clusterHitMap\n",
    "                            #hit map takes the 7 hits after the lowest pixel index\n",
    "                            lowPixIdx = pixelAddr(lowPixAddrHit.Pixel_x, lowPixAddrHit.Pixel_z)\n",
    "                            encoderID = pEncoderID(lowPixAddrHit.Pixel_z)\n",
    "                            addr = pixelAddr(lowPixAddrHit.Pixel_x, lowPixAddrHit.Pixel_z)\n",
    "                            hitMap = 7*[0]\n",
    "                            for idx, j in enumerate(range(lowPixIdx+1, lowPixIdx+8)):\n",
    "                                #this has to happen only on the same priority encoder...\n",
    "                                for hit in PEHits[1:]:\n",
    "                                    if (pixelAddr(hit.Pixel_x, hit.Pixel_z) == j ):\n",
    "                                        hitMap[idx] = 1\n",
    "                                        break\n",
    "                            #format hitmap as Little Endian 7 bit word\n",
    "                            hitMap = int(\"\".join(str(x) for x in hitMap),2)\n",
    "                            hitMap = f'{hitMap:07b}'\n",
    "                            hitMap = hitMap[::-1]\n",
    "                            # note from Yasser: if hitmap == f'{0:07b}' we just write DataShort\n",
    "                            # I handle that later in the code\n",
    "                            DATA_SHORT = [encoderID, addr]\n",
    "                            data = [DATA_SHORT, hitMap]\n",
    "\n",
    "                            \n",
    "                        # without clustering we read out every pixel hit as data_shorts\n",
    "                        else:\n",
    "                            print(\"noClustering\")\n",
    "                            for hit in PEHits:\n",
    "                                encoderID = pEncoderID(hit.Pixel_z)\n",
    "                                addr = pixelAddr(hit.Pixel_x, hit.Pixel_z)\n",
    "                                DATA_SHORT = [encoderID, addr]\n",
    "                                data.append(DATA_SHORT)\n",
    "                                \n",
    "                    # empty PE\n",
    "                    else:\n",
    "                        DATA_SHORT = []\n",
    "                        if(EnableClustering):\n",
    "                            data = [DATA_SHORT,'']\n",
    "                    \n",
    "                    localRegionData.append(data)\n",
    "                    data = []\n",
    "                \n",
    "                # set chip regionData\n",
    "                self.regionData.append(RegionInfo(PEHitList.copy(), localRegionData.copy(), i))\n",
    "                localRegionData.clear()\n",
    "                PEHitList.clear()\n",
    "            \n",
    "            # empty chips get empty chip header on init\n",
    "            if self.isEmpty():\n",
    "                print(\"chip\" + self.Chip + \"empty... no region readout...\")\n",
    "                return\n",
    "            \n",
    "            # now format the binary region data...\n",
    "            # regions are read in parallel by round robin... \n",
    "            # ignore sim of time ordering, no shuffling...\n",
    "            # random.shuffle(self.regionData)\n",
    "            \n",
    "            # Chip Data Header\n",
    "            # TODO currently there isn't a bunch crossing counter in the simulation data....\n",
    "            BCC = '00000000'\n",
    "            chipDataHeader = '1010' + f'{self.Chip:04b}' + BCC\n",
    "            chipDataHeaderBytes = bytearray([int(chipDataHeader[0:8],2),int(chipDataHeader[8:16],2)])\n",
    "            self.byteData = self.byteData + chipDataHeaderBytes\n",
    "            \n",
    "            #Chip Region Data\n",
    "            for region in self.regionData:\n",
    "                if region.isEmpty():\n",
    "                    continue\n",
    "                regionBytes = region.getFormattedData()\n",
    "                self.byteData = self.byteData + regionBytes\n",
    "            \n",
    "            # Chip Data Trailer\n",
    "            # TODO readoutflags not currently in the simulation....\n",
    "            readout_flags = '0000'\n",
    "            chipDataTrailer = '1011' + readout_flags\n",
    "            chipDataTrailerBytes = bytearray([int(chipDataTrailer,2)])\n",
    "            self.byteData = self.byteData + chipDataTrailerBytes\n",
    "            \n",
    "            #align to 9 bytes for RU formatting- remainder data is IDLE word: FF\n",
    "            if(len(self.byteData) != 9):\n",
    "                remainder = 9 - (len(self.byteData) % 9)\n",
    "                if(remainder):\n",
    "                    for i in range (0, remainder):\n",
    "                        self.byteData = self.byteData + bytearray([255])  \n",
    "            self.formatted = True\n",
    "            return\n",
    "\n",
    "    def Print(self):\n",
    "        if self.isEmpty():\n",
    "            return\n",
    "        print(\"|------- Chip Info -------\")\n",
    "        print(\"Chip: \", self.Chip, \" Region Hit Lists...\")\n",
    "        print()\n",
    "        for region in self.regionData:\n",
    "            region.Print()\n",
    "        print()\n",
    "        \n",
    "    def PrintBytes(self):\n",
    "        if self.isEmpty():\n",
    "            return\n",
    "        print(\"|------- Chip: \" , self.Chip , \" ByteInfo -------\")\n",
    "        print( ' '.join( '{:02x}'.format(x) for x in self.getFormattedData() ) )\n",
    "        print()\n",
    "\n",
    "     \n",
    "    def isEmpty(self):\n",
    "        for region in self.regionData:\n",
    "            if(not region.isEmpty()):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    ALPIDE_REGIONS = 32\n",
    "    PENCODER_PER_REGION = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a5220",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionInfo:\n",
    "    def __init__(self, PEHitList, Data, RegionNo):\n",
    "        self.PEHitList = PEHitList\n",
    "        self.Data = Data\n",
    "        self.RegionNo = RegionNo\n",
    "        self.byteData = bytearray([])\n",
    "        self.formatted = False\n",
    "        \n",
    "    def getFormattedData(self):\n",
    "        if not self.formatted:\n",
    "            self.formatData()\n",
    "        return self.byteData\n",
    "\n",
    "    def formatData(self):\n",
    "        # Readout PE in sequence....\n",
    "        # Data is Region header + PE words if hits\n",
    "        if self.isEmpty():\n",
    "            return\n",
    "        regionHeader = '110' + f'{self.RegionNo:05b}'\n",
    "        tempBinData = regionHeader\n",
    "        for PE , hits in enumerate(self.PEHitList):\n",
    "            if hits:\n",
    "                if(EnableClustering):\n",
    "                    DATA_LONG = self.Data[PE]\n",
    "                    DATA_SHORT = DATA_LONG[0]\n",
    "                    hit_map = DATA_LONG[1]\n",
    "                    encoder_id = int(DATA_SHORT[0])\n",
    "                    addr = int(DATA_SHORT[1])\n",
    "                    # zero suppress non-clusters...\n",
    "                    if (hit_map == f'{0:07b}'):\n",
    "                        #send data short - header is 01 not 00\n",
    "                        binDataLong = '01' + f'{encoder_id:04b}' + f'{addr:010b}'\n",
    "                    else:\n",
    "                        binDataLong = '00' + f'{encoder_id:04b}' + f'{addr:010b}' + '0' + hit_map\n",
    "                    tempBinData = tempBinData + binDataLong\n",
    "                    \n",
    "                #otherwise we have many DATA_SHORT to read...\n",
    "                else:\n",
    "                    DATA_SHORTS = self.Data[PE]\n",
    "                    for DATA_SHORT in DATA_SHORTS:\n",
    "                        encoder_id = int(DATA_SHORT[0])\n",
    "                        addr = int(DATA_SHORT[1])\n",
    "                        binDataShort = '01' + f'{encoder_id:04b}' + f'{addr:010b}'\n",
    "                        tempBinData = tempBinData + binDataShort\n",
    "                    \n",
    "        # break bin data into bytes....\n",
    "        self.byteData = toBytes(tempBinData)  \n",
    "        self.formatted = True\n",
    "                \n",
    "    def Print(self):\n",
    "        if self.isEmpty():\n",
    "            return\n",
    "        print(\"-------- Region: \", self.RegionNo, \"--------\")\n",
    "        for PE, hits in enumerate(self.PEHitList):\n",
    "            if hits:\n",
    "                print(\"-------- Priority Encoder: \" , PE , \"--------\")\n",
    "                for hit in hits:\n",
    "                    hit.Print()\n",
    "                if(EnableClustering):\n",
    "                    DATA_LONG = self.Data[PE]\n",
    "                    DATA_SHORT = DATA_LONG[0]\n",
    "                    ClusterMap = DATA_LONG[1]\n",
    "                    print(\"ClusterMap: \", ClusterMap)\n",
    "                    print(\"Data_SHORT: \", DATA_SHORT)\n",
    "                else:\n",
    "                    DATA_SHORTS = self.Data[PE]\n",
    "                    for DATA_SHORT in DATA_SHORTS:\n",
    "                        print(\"DATA_SHORT: \", DATA_SHORT)\n",
    "                print()\n",
    "                \n",
    "    def PrintBytes(self):\n",
    "        if self.isEmpty():\n",
    "            return\n",
    "        print(\"-------- Region: \", self.RegionNo, \"--------\")\n",
    "        print( ' '.join( '{:02x}'.format(x) for x in self.getFormattedData() ) )\n",
    "        print()\n",
    "        \n",
    "    def isEmpty(self):\n",
    "        for PE in self.PEHitList:\n",
    "            if len(PE) != 0:\n",
    "               return False \n",
    "        return True     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9734e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "MVTXLayers = 3\n",
    "ChipsPerStave = 9\n",
    "EnableClustering = True\n",
    "\n",
    "datafd = open('Signal.json')\n",
    "outfd = open('Signal_gbt.bin','w')\n",
    "data = json.load(datafd)\n",
    "\n",
    "EventGBTinfo = []\n",
    "AlpideData = []\n",
    "StaveData = []\n",
    "HitList = []\n",
    "print(\"TotalEvents:\", len(data['Events']))\n",
    "print()\n",
    "\n",
    "for evidx, ev in enumerate(data['Events']):\n",
    "    \n",
    "    print(\"--------------------\")        \n",
    "    print(\"--------------------\")    \n",
    "    print(\"Start of event: \", evidx)\n",
    "    print(\"--------------------\")\n",
    "    print(\"--------------------\")   \n",
    "    print()\n",
    "    \n",
    "    EventGBTinfo.clear()\n",
    "    AlpideData.clear()\n",
    "    StaveData.clear()\n",
    "    HitList.clear()\n",
    "    RawHit = ev['RawHit']\n",
    "    MVTXHits = RawHit['MVTXHits']\n",
    "    \n",
    "    # pull mvtx hit data info \n",
    "    for mvtxhit in MVTXHits:\n",
    "        hitinfo = mvtxhit['ID']\n",
    "        if(hitinfo['MVTXTrkID'] > 0):\n",
    "            HitList.append(Hit(hitinfo['MVTXTrkID'],hitinfo['Layer'],hitinfo['Stave'],hitinfo['Chip'],\n",
    "                                         hitinfo['Pixel_x'], hitinfo['Pixel_z']))\n",
    "    \n",
    "            \n",
    "    # geographic grouping of Hits by Layer, Stave:\n",
    "    for layer in range(0,MVTXLayers):\n",
    "        layerHitList = [x for x in HitList if x.Layer == layer]\n",
    "        stavesInLayer = unique([x.Stave for x in layerHitList])\n",
    "        for stave in stavesInLayer:\n",
    "            layerStaveHitList = [x for x in layerHitList if x.Stave == stave]\n",
    "            # reduce data on ChipID \n",
    "            for ChipNo in range(0,ChipsPerStave):\n",
    "                ChipHits = [x for x in layerStaveHitList if x.Chip == ChipNo]\n",
    "                if ChipHits:\n",
    "                    SampleHit = ChipHits[0]\n",
    "                    AlpideData.append(AlpideInfo(SampleHit.Layer,SampleHit.Stave,SampleHit.Chip,\n",
    "                                         ChipHits))\n",
    "                #else add empty alpide chip - empty chips still read out empty chip header later on...\n",
    "                else:\n",
    "                    AlpideData.append(AlpideInfo(-1,-1,ChipNo,[]))\n",
    "            StaveData.append(StaveInfo(layer,stave,AlpideData))\n",
    "            AlpideData.clear()\n",
    "\n",
    "            \n",
    "    # format 9 alpide worth of data for each stave for sending to RU     \n",
    "    for stave in StaveData:\n",
    "        stave.formatData()\n",
    "        stave.Print()\n",
    "        print(\"--------------------------------\")\n",
    "        stave.PrintChips()\n",
    "        stave.PrintGBTLinks()\n",
    "        \n",
    "    print(\"------------\")        \n",
    "    print(\"------------\")    \n",
    "    print(\"end of event\")\n",
    "    print(\"------------\")\n",
    "    print(\"------------\")\n",
    "    print()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
